{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST('', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "test = datasets.MNIST('', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x): #how data will flow\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1) #dim is similar to axes, which thing we want to sum to 1, classes, dim=0 batches, dim=1 output tensors\n",
    "        \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(-1, 28*28) #-1 or 1 specifies this input is of unknown shape, any amount of data, any size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3701e-01, 8.6258e-01, 8.1131e-01, 3.3978e-01, 5.6041e-01, 1.7871e-01,\n",
       "         7.0201e-01, 9.0366e-01, 2.3984e-01, 6.4894e-01, 8.9459e-01, 2.0224e-01,\n",
       "         3.0065e-01, 9.3213e-01, 4.1174e-01, 6.1911e-01, 3.2761e-02, 8.6420e-01,\n",
       "         2.9420e-01, 7.5112e-01, 3.1752e-01, 2.3166e-01, 1.3140e-01, 2.1826e-01,\n",
       "         6.3806e-01, 3.4232e-01, 9.8145e-01, 9.1253e-01, 3.7485e-01, 5.9274e-01,\n",
       "         5.3883e-01, 4.8782e-01, 7.6988e-01, 9.3271e-01, 2.2220e-01, 9.7222e-01,\n",
       "         7.2229e-01, 4.9664e-01, 1.4347e-01, 9.2758e-01, 9.5139e-01, 2.7097e-01,\n",
       "         7.9688e-01, 7.7217e-01, 9.9675e-01, 8.1293e-02, 6.9044e-01, 4.1908e-01,\n",
       "         2.6300e-01, 5.4073e-01, 5.4939e-01, 6.6409e-01, 2.6812e-01, 6.3764e-01,\n",
       "         2.3129e-01, 1.5799e-01, 7.6656e-01, 6.1866e-01, 7.8757e-02, 9.6516e-01,\n",
       "         6.4713e-01, 6.4285e-01, 2.7198e-01, 5.2218e-01, 4.7181e-01, 5.8316e-01,\n",
       "         7.9755e-01, 7.6465e-01, 6.6418e-01, 2.2116e-01, 8.9862e-01, 9.6037e-01,\n",
       "         1.0097e-01, 1.4093e-01, 7.4492e-01, 7.4432e-01, 7.8068e-02, 2.0214e-01,\n",
       "         2.7408e-01, 9.9937e-01, 5.0325e-01, 4.8486e-01, 2.5226e-01, 7.2865e-02,\n",
       "         7.3636e-01, 9.0256e-01, 7.3694e-01, 6.1091e-01, 4.7779e-02, 9.8999e-01,\n",
       "         8.6844e-02, 5.5876e-01, 3.2715e-01, 3.9392e-01, 9.3356e-01, 6.6994e-01,\n",
       "         5.6916e-01, 1.6261e-01, 5.8528e-01, 3.0869e-01, 1.9498e-02, 3.2911e-01,\n",
       "         8.5356e-01, 3.9205e-01, 3.9305e-01, 3.7976e-01, 1.3317e-01, 5.1024e-01,\n",
       "         9.7083e-01, 1.8405e-01, 7.1911e-01, 5.1192e-01, 9.8391e-01, 7.8352e-02,\n",
       "         1.5807e-01, 1.9766e-01, 2.4692e-01, 4.4353e-01, 2.2368e-01, 4.6772e-01,\n",
       "         3.3350e-01, 6.4096e-02, 9.8662e-01, 9.0583e-01, 7.0396e-01, 1.4662e-01,\n",
       "         6.4325e-01, 8.1567e-01, 8.2218e-01, 3.9638e-02, 7.2995e-01, 5.1738e-01,\n",
       "         7.7202e-01, 4.4043e-01, 7.2791e-01, 9.8454e-01, 8.3015e-01, 3.6376e-01,\n",
       "         5.3165e-01, 7.5892e-01, 3.3448e-01, 3.9697e-02, 3.0079e-01, 5.6138e-01,\n",
       "         7.9111e-01, 5.8831e-01, 6.1308e-01, 2.6607e-02, 2.6275e-01, 7.3499e-01,\n",
       "         1.2771e-01, 6.9004e-01, 2.6510e-01, 4.8079e-01, 7.9076e-01, 2.6779e-01,\n",
       "         4.8088e-01, 7.3447e-01, 6.3449e-01, 6.6942e-01, 3.1432e-01, 3.5221e-02,\n",
       "         2.3879e-02, 8.8005e-01, 7.1358e-01, 7.4654e-02, 5.0933e-01, 1.2942e-01,\n",
       "         6.1969e-01, 9.9509e-01, 6.7638e-01, 1.2676e-01, 4.3550e-01, 8.1530e-02,\n",
       "         4.4609e-01, 5.8084e-01, 9.9764e-01, 2.4365e-01, 6.6522e-01, 4.4979e-01,\n",
       "         1.7894e-01, 4.2915e-01, 6.6198e-01, 2.7063e-01, 5.9897e-01, 4.7405e-01,\n",
       "         4.7780e-02, 6.8670e-02, 4.2917e-01, 3.2052e-01, 1.5072e-01, 3.9615e-02,\n",
       "         3.6430e-01, 9.2943e-01, 4.1147e-01, 4.9160e-02, 9.9714e-01, 8.2770e-01,\n",
       "         2.2246e-01, 7.4643e-01, 2.0145e-01, 8.9682e-01, 4.5931e-01, 9.5769e-01,\n",
       "         1.5949e-01, 7.0992e-01, 9.8941e-01, 7.9529e-01, 5.1587e-01, 4.4017e-01,\n",
       "         3.8185e-01, 4.1750e-01, 4.5992e-01, 7.1108e-05, 2.2343e-01, 1.4023e-01,\n",
       "         3.9440e-01, 1.4381e-01, 7.7630e-01, 2.2701e-01, 5.8983e-01, 8.0760e-01,\n",
       "         4.1482e-01, 8.6136e-01, 6.4018e-01, 8.8046e-01, 2.5561e-01, 6.6421e-01,\n",
       "         2.6565e-01, 7.7167e-01, 1.8847e-01, 1.2893e-01, 4.2715e-01, 7.9154e-01,\n",
       "         5.2630e-01, 6.8840e-01, 1.8007e-01, 7.5633e-01, 3.2059e-01, 3.1954e-01,\n",
       "         8.3360e-02, 6.2698e-01, 1.2882e-01, 1.6905e-01, 3.3065e-01, 2.6498e-01,\n",
       "         7.7994e-01, 4.9993e-01, 9.8936e-01, 5.6040e-01, 7.1187e-01, 8.6792e-01,\n",
       "         2.5047e-01, 2.5863e-01, 2.4039e-01, 2.7170e-01, 5.2048e-01, 2.9220e-01,\n",
       "         7.5034e-01, 2.4457e-01, 2.6674e-01, 6.6118e-01, 8.3545e-03, 5.1060e-01,\n",
       "         4.3031e-01, 2.4650e-01, 4.2348e-01, 1.3771e-01, 5.7223e-01, 4.1785e-01,\n",
       "         7.0371e-01, 2.9517e-01, 1.3882e-01, 8.5075e-01, 5.6938e-01, 1.8372e-01,\n",
       "         5.4945e-01, 5.5557e-01, 9.9926e-02, 2.9532e-02, 8.9753e-01, 5.3733e-01,\n",
       "         3.1711e-02, 5.7029e-01, 7.9852e-01, 3.7322e-01, 3.5918e-01, 2.2075e-01,\n",
       "         9.7230e-03, 4.1851e-02, 7.9911e-01, 3.6293e-01, 9.8068e-01, 3.0363e-01,\n",
       "         9.4269e-02, 6.1017e-01, 3.3703e-01, 7.8667e-01, 6.9328e-01, 9.7279e-01,\n",
       "         7.1911e-01, 9.5825e-01, 2.0529e-01, 6.8184e-01, 3.6560e-01, 7.8684e-01,\n",
       "         9.9353e-01, 4.4599e-01, 5.8049e-01, 1.7399e-01, 8.0076e-01, 8.7534e-01,\n",
       "         9.0044e-01, 8.6268e-02, 7.8376e-02, 9.9886e-01, 6.6812e-01, 5.5012e-01,\n",
       "         7.6722e-01, 4.6585e-01, 9.6027e-01, 7.9294e-01, 5.8605e-01, 8.6020e-01,\n",
       "         7.4456e-01, 3.4390e-01, 1.4173e-01, 4.8260e-01, 9.3714e-01, 9.4080e-01,\n",
       "         8.1939e-01, 9.3641e-01, 8.7952e-01, 1.8591e-01, 7.8757e-01, 2.7156e-02,\n",
       "         8.1730e-01, 5.4424e-01, 6.3997e-01, 4.6036e-01, 5.8382e-01, 9.7436e-01,\n",
       "         6.2529e-01, 9.3422e-01, 4.3285e-02, 4.8663e-01, 8.2620e-02, 8.1108e-01,\n",
       "         7.7057e-01, 2.8765e-01, 9.1371e-01, 8.4270e-01, 5.0914e-02, 6.9224e-02,\n",
       "         2.7349e-01, 9.1955e-01, 5.7162e-01, 7.2403e-01, 2.5532e-03, 5.3671e-01,\n",
       "         9.3173e-01, 8.9690e-01, 8.1836e-01, 3.0572e-01, 3.9254e-01, 5.5927e-01,\n",
       "         1.7999e-01, 8.0196e-02, 5.7231e-01, 3.0286e-01, 1.6861e-02, 9.5204e-01,\n",
       "         1.1663e-02, 5.1187e-01, 2.3220e-02, 8.4969e-01, 9.4750e-01, 2.0546e-01,\n",
       "         7.0404e-01, 3.0164e-01, 9.7378e-01, 5.4027e-01, 6.5194e-01, 2.4193e-02,\n",
       "         6.1206e-01, 8.2300e-01, 6.8698e-01, 6.2134e-01, 9.0171e-02, 2.7102e-01,\n",
       "         1.4419e-01, 6.8638e-01, 4.0077e-01, 6.3339e-01, 2.3651e-01, 9.1004e-01,\n",
       "         7.7393e-01, 1.2820e-01, 2.5490e-01, 9.0799e-02, 2.2687e-01, 3.2865e-01,\n",
       "         9.3293e-01, 8.9195e-01, 1.2253e-01, 2.0698e-01, 2.0199e-01, 9.8843e-01,\n",
       "         9.3196e-01, 6.6215e-01, 5.1102e-01, 1.5579e-01, 8.9330e-01, 5.8159e-01,\n",
       "         8.1502e-01, 2.7292e-01, 8.7599e-02, 2.0564e-01, 1.9175e-01, 4.3964e-01,\n",
       "         4.8149e-01, 4.1258e-01, 6.7505e-01, 6.1989e-01, 7.9175e-01, 7.0475e-01,\n",
       "         8.4998e-01, 1.9628e-01, 8.9429e-01, 2.6469e-01, 2.3118e-02, 3.6068e-01,\n",
       "         4.5671e-01, 8.2480e-01, 2.5374e-01, 9.7433e-01, 3.4621e-01, 4.4520e-01,\n",
       "         2.5923e-01, 1.9507e-01, 1.4240e-01, 9.0371e-01, 1.1039e-01, 1.2753e-01,\n",
       "         2.1438e-01, 2.8247e-01, 1.8468e-01, 9.0895e-02, 6.0653e-01, 1.1889e-01,\n",
       "         8.7170e-02, 8.8126e-01, 4.2754e-01, 3.7781e-01, 9.7758e-01, 3.7082e-02,\n",
       "         1.7137e-01, 7.8794e-01, 5.7343e-03, 7.0298e-01, 8.0802e-01, 1.9354e-01,\n",
       "         7.9915e-01, 5.2776e-02, 1.0874e-01, 2.2809e-01, 9.8321e-01, 5.3025e-01,\n",
       "         5.9647e-01, 6.9098e-01, 8.6601e-01, 2.8257e-01, 1.3207e-01, 8.6677e-01,\n",
       "         7.8613e-01, 6.2928e-01, 5.9167e-01, 6.5958e-01, 7.2831e-01, 2.9719e-01,\n",
       "         2.5982e-01, 8.6781e-01, 3.8489e-01, 2.9526e-01, 4.5520e-01, 6.9588e-01,\n",
       "         3.0119e-01, 2.3747e-01, 5.8990e-01, 3.1587e-01, 2.6481e-01, 5.7078e-01,\n",
       "         9.9685e-01, 9.3546e-01, 7.0481e-01, 3.9747e-01, 6.4512e-01, 7.7125e-01,\n",
       "         4.2686e-01, 3.9877e-01, 4.1897e-02, 2.4949e-01, 2.9437e-01, 8.1864e-02,\n",
       "         2.3915e-01, 5.6497e-02, 5.1747e-01, 7.3281e-01, 5.9094e-02, 6.6316e-02,\n",
       "         2.7118e-01, 2.1402e-02, 5.9674e-01, 6.2104e-01, 4.7606e-01, 3.7484e-01,\n",
       "         2.8597e-01, 4.5505e-01, 6.2420e-01, 8.0038e-01, 4.2188e-01, 2.1518e-01,\n",
       "         2.0933e-01, 8.1731e-01, 1.3133e-01, 9.9862e-01, 7.3363e-01, 2.8082e-01,\n",
       "         3.4117e-01, 3.7491e-01, 7.0984e-01, 5.5508e-01, 8.1832e-01, 1.8052e-01,\n",
       "         9.2090e-01, 2.0175e-01, 8.2905e-01, 4.5815e-02, 3.6886e-01, 7.9755e-01,\n",
       "         4.6584e-01, 1.2317e-01, 8.7525e-01, 9.6915e-01, 3.9263e-01, 8.8802e-01,\n",
       "         9.7742e-01, 5.4690e-01, 9.5600e-01, 5.5412e-01, 3.9334e-01, 6.0768e-01,\n",
       "         4.0985e-01, 6.9834e-02, 7.2612e-01, 5.9928e-01, 3.2478e-01, 9.7718e-01,\n",
       "         3.0800e-01, 5.8906e-02, 5.7027e-02, 4.8499e-01, 4.0196e-01, 1.0447e-01,\n",
       "         2.1452e-01, 7.8488e-01, 5.3626e-01, 8.0099e-01, 4.9168e-01, 7.6188e-01,\n",
       "         1.8372e-02, 7.9387e-01, 4.4330e-01, 2.4930e-01, 7.8683e-01, 8.6756e-01,\n",
       "         5.7811e-01, 7.6715e-01, 6.1648e-01, 5.5912e-01, 8.0328e-01, 6.9813e-01,\n",
       "         1.6629e-01, 7.7592e-01, 5.8830e-02, 4.1071e-01, 3.4282e-01, 6.6462e-01,\n",
       "         7.8690e-01, 5.5605e-01, 7.3141e-01, 6.5509e-03, 9.2794e-01, 4.2332e-01,\n",
       "         4.1445e-01, 7.0552e-01, 6.4742e-01, 4.0213e-01, 2.3583e-01, 1.4372e-02,\n",
       "         3.1629e-01, 2.0520e-01, 4.4185e-01, 9.8883e-01, 8.9690e-01, 9.4025e-02,\n",
       "         1.9059e-01, 4.5661e-01, 9.8888e-01, 2.4737e-01, 9.0886e-01, 8.1587e-04,\n",
       "         7.2349e-01, 8.5046e-01, 9.5540e-01, 4.6725e-01, 6.4641e-02, 4.9038e-01,\n",
       "         8.9605e-01, 2.7992e-01, 4.5836e-01, 8.7978e-01, 7.0114e-01, 3.4220e-01,\n",
       "         8.5204e-01, 5.5049e-01, 1.7849e-01, 1.5257e-01, 1.9478e-01, 5.0128e-01,\n",
       "         3.9875e-01, 8.9522e-01, 9.7501e-01, 6.4660e-01, 3.7281e-01, 2.3498e-02,\n",
       "         1.9652e-01, 4.4956e-01, 6.5147e-02, 1.4136e-02, 3.0429e-01, 5.1241e-01,\n",
       "         9.4168e-01, 5.6355e-01, 6.4134e-02, 9.1951e-01, 2.7075e-01, 6.1495e-01,\n",
       "         1.1200e-01, 7.8926e-01, 5.7522e-01, 5.8492e-01, 5.7265e-01, 9.0089e-01,\n",
       "         5.7489e-01, 1.1616e-03, 3.4617e-01, 5.5567e-01, 7.6091e-01, 8.6685e-01,\n",
       "         1.6113e-01, 5.5513e-01, 3.1090e-01, 8.9488e-02, 7.7043e-01, 8.1665e-01,\n",
       "         3.9993e-01, 3.1285e-02, 4.3936e-02, 5.3324e-01, 7.7446e-01, 5.4776e-01,\n",
       "         8.7791e-01, 7.5992e-02, 4.6681e-01, 5.8628e-01, 1.2170e-01, 8.9205e-01,\n",
       "         2.6922e-01, 5.8503e-01, 2.1700e-01, 6.4146e-02, 9.8703e-01, 6.8107e-01,\n",
       "         7.5841e-01, 7.9368e-01, 6.2359e-01, 1.5920e-01, 8.6424e-01, 3.4948e-01,\n",
       "         1.3569e-01, 2.4114e-01, 2.9987e-01, 8.1199e-01, 3.3919e-01, 1.3897e-02,\n",
       "         3.5013e-01, 7.5503e-01, 5.1961e-01, 1.4309e-01, 4.3220e-01, 1.6678e-01,\n",
       "         9.0864e-01, 8.9721e-01, 4.4140e-01, 2.8401e-01, 1.7106e-02, 2.7683e-01,\n",
       "         4.8748e-01, 7.9596e-01, 4.7586e-01, 4.3373e-01, 3.6402e-01, 3.8161e-02,\n",
       "         8.7131e-01, 3.2477e-01, 3.5173e-01, 4.0800e-01, 1.5159e-01, 7.1786e-01,\n",
       "         8.4851e-01, 4.4985e-02, 2.2261e-01, 8.1891e-01, 6.4919e-01, 1.6496e-01,\n",
       "         6.0992e-01, 1.3122e-01, 7.7655e-01, 1.0246e-01, 9.4514e-01, 8.1382e-01,\n",
       "         4.5012e-01, 1.8844e-01, 6.3576e-01, 2.4928e-01, 3.5421e-01, 9.2192e-01,\n",
       "         6.0490e-01, 5.8975e-03, 6.6159e-01, 9.0411e-01, 8.0031e-01, 8.5275e-01,\n",
       "         6.5630e-01, 8.4692e-01, 3.6550e-01, 8.9140e-01, 2.8780e-01, 6.6071e-01,\n",
       "         5.5015e-01, 6.1505e-01, 3.7414e-01, 8.7471e-01, 3.5268e-01, 6.0301e-02,\n",
       "         2.5017e-01, 2.5433e-01, 8.7197e-01, 9.9913e-01, 4.6469e-02, 1.1687e-01,\n",
       "         5.2421e-01, 8.6648e-01, 1.0136e-01, 5.1246e-01, 6.2488e-02, 5.8912e-01,\n",
       "         1.0733e-01, 7.1634e-01, 8.3337e-01, 2.5106e-01, 3.4303e-01, 9.1941e-01,\n",
       "         7.0733e-01, 4.1564e-01, 1.5609e-01, 1.2710e-01, 9.4736e-01, 2.0743e-02,\n",
       "         3.8625e-01, 7.5043e-01, 5.1133e-02, 9.3676e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3951, -2.2063, -2.1525, -2.3594, -2.2815, -2.2574, -2.4473, -2.1634,\n",
       "         -2.4161, -2.4017]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3224, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1807, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr =  0.001)\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        #data is a batch of featuresets and labels\n",
    "        X,y = data\n",
    "        #print(X[0])\n",
    "        #print(y[0])\n",
    "        #break\n",
    "        net.zero_grad() #eveytime we pass data through network, we need to start with this\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        loss = F.nll_loss(output, y) #nll_loss for scalar data lables, mean square for one-hot\n",
    "        loss.backward()\n",
    "        optimizer.step() #will adjust weights for us    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.98\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    #for validation we dont want to calculate gradients\n",
    "    #for every pred does it match actual label\n",
    "    for data in trainset:\n",
    "        X,y = data\n",
    "        output = net(X.view(-1, 784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct +=1\n",
    "            total += 1\n",
    "print (\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.971\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    #for validation we dont want to calculate gradients\n",
    "    #for every pred does it match actual label\n",
    "    for data in testset:\n",
    "        X,y = data\n",
    "        output = net(X.view(-1, 784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct +=1\n",
    "            total += 1\n",
    "print (\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X[1].view(28,28))\n",
    "plt.show() #actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[1].view(-1,784))[0])) #prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
